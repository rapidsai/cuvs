/*
 * Copyright (c) 2025, NVIDIA CORPORATION.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.nvidia.cuvs.internal.common;

import static com.nvidia.cuvs.internal.common.LinkerHelper.C_CHAR;
import static com.nvidia.cuvs.internal.common.LinkerHelper.C_FLOAT;
import static com.nvidia.cuvs.internal.common.LinkerHelper.C_INT;
import static com.nvidia.cuvs.internal.common.LinkerHelper.C_LONG;
import static com.nvidia.cuvs.internal.panama.headers_h.*;
import static com.nvidia.cuvs.internal.panama.headers_h_1.cudaStream_t;

import com.nvidia.cuvs.CuVSResources;
import com.nvidia.cuvs.GPUInfo;
import com.nvidia.cuvs.internal.panama.DLDataType;
import com.nvidia.cuvs.internal.panama.DLDevice;
import com.nvidia.cuvs.internal.panama.DLManagedTensor;
import com.nvidia.cuvs.internal.panama.DLTensor;
import com.nvidia.cuvs.internal.panama.cudaDeviceProp;
import com.nvidia.cuvs.internal.panama.headers_h;
import java.lang.foreign.Arena;
import java.lang.foreign.Linker;
import java.lang.foreign.MemoryLayout;
import java.lang.foreign.MemoryLayout.PathElement;
import java.lang.foreign.MemorySegment;
import java.lang.invoke.MethodHandle;
import java.lang.invoke.VarHandle;
import java.util.ArrayList;
import java.util.BitSet;
import java.util.List;

public class Util {

  public static final int CUVS_SUCCESS = headers_h.CUVS_SUCCESS();
  public static final int CUDA_SUCCESS = 0;

  private Util() {}

  private static final Linker LINKER = Linker.nativeLinker();

  /**
   * Bindings for {@code cudaMemcpyAsync}; differently from the {@code headers_h} bindings (which are
   * automatically generated by {@code jextract}), these bindings specify the {@code critical} linker option,
   * which allows to directly access heap-based {@link MemorySegment}s.
   * {@snippet lang=c :
   * extern cudaError_t cudaMemcpyAsync(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind, cudaStream_t stream)
   * }
   */
  private static final MethodHandle cudaMemcpyAsync$mh =
      LINKER.downcallHandle(
          cudaMemcpyAsync$address(), cudaMemcpyAsync$descriptor(), Linker.Option.critical(true));

  /**
   * Checks the result value of a (CuVS) native method handle call.
   *
   * @param value  the return value
   * @param caller the native method handle that was called
   */
  public static void checkCuVSError(int value, String caller) {
    if (value != CUVS_SUCCESS) {
      String errorMsg = getLastErrorText();
      throw new RuntimeException(caller + " returned " + value + "[" + errorMsg + "]");
    }
  }

  /**
   * Checks the result value of a (CUDA) native method handle call.
   *
   * @param value  the return value
   * @param caller the native method handle that was called
   */
  public static void checkCudaError(int value, String caller) {
    if (value != CUDA_SUCCESS) {
      throw new RuntimeException(caller + " returned " + value);
    }
  }

  private static final long UNSIGNED_INT_MASK = 0xFFFFFFFFL;

  public static long dereferenceUnsignedInt(MemorySegment ptr) {
    assert ptr.byteSize() == 4;
    return ptr.get(uint32_t, 0) & UNSIGNED_INT_MASK;
  }

  /**
   * Java analog to CUDA's cudaMemcpyKind, used for cudaMemcpy() calls.
   * @see <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html">CUDA Runtime API</a>
   */
  public enum CudaMemcpyKind {
    HOST_TO_HOST(cudaMemcpyHostToHost()),
    HOST_TO_DEVICE(cudaMemcpyHostToDevice()),
    DEVICE_TO_HOST(cudaMemcpyDeviceToHost()),
    DEVICE_TO_DEVICE(cudaMemcpyDeviceToDevice()),
    INFER_DIRECTION(4);

    CudaMemcpyKind(int k) {
      this.kind = k;
    }

    public final int kind;
  }

  /**
   * Helper to invoke cudaMemcpy CUDA runtime function to copy data between host/device memory.
   * @param dest Destination address for data copy
   * @param src Source address for data copy
   * @param numBytes Number of bytes to be copied
   * @param kind "Direction" of data copy (Host->Device, Device->Host, etc.)
   * @throws RuntimeException on failure of copy
   */
  public static void cudaMemcpy(
      MemorySegment dest, MemorySegment src, long numBytes, CudaMemcpyKind kind) {
    int returnValue =
        com.nvidia.cuvs.internal.panama.headers_h.cudaMemcpy(dest, src, numBytes, kind.kind);
    checkCudaError(returnValue, "cudaMemcpy");
  }

  /**
   * Helper to invoke cudaMemcpy CUDA runtime function to copy data between host/device memory.
   * @param dest Destination address for data copy
   * @param src Source address for data copy
   * @param numBytes Number of bytes to be copied
   * @throws RuntimeException on failure of copy
   */
  public static void cudaMemcpy(MemorySegment dest, MemorySegment src, long numBytes) {
    Util.cudaMemcpy(dest, src, numBytes, CudaMemcpyKind.INFER_DIRECTION);
  }

  /**
   * Helper to invoke cudaMemcpyAsync CUDA runtime function to copy data between host/device memory.
   *
   * @param dst Destination address for data copy
   * @param src Source address for data copy
   * @param numBytes Number of bytes to be copied
   * @param kind "Direction" of data copy (Host->Device, Device->Host, etc.)
   * @throws RuntimeException on failure of copy
   */
  public static void cudaMemcpyAsync(
      MemorySegment dst,
      MemorySegment src,
      long numBytes,
      CudaMemcpyKind kind,
      MemorySegment stream) {
    try {
      int returnValue = (int) cudaMemcpyAsync$mh.invokeExact(dst, src, numBytes, kind.kind, stream);
      checkCudaError(returnValue, "cudaMemcpyAsync");
    } catch (Throwable ex$) {
      throw new AssertionError("should not reach here", ex$);
    }
  }

  /**
   * Helper to get the CUDA stream associated with a {@link CuVSResources}
   */
  public static MemorySegment getStream(CuVSResources resources) {
    try (var resourcesAccess = resources.access();
        var localArena = Arena.ofConfined()) {
      var streamPointer = localArena.allocate(cudaStream_t);
      checkCuVSError(cuvsStreamGet(resourcesAccess.handle(), streamPointer), "cuvsStreamGet");
      return streamPointer.get(cudaStream_t, 0);
    }
  }

  static final long MAX_ERROR_TEXT = 1_000_000L;

  static String getLastErrorText() {
    try {
      var seg = headers_h.cuvsGetLastErrorText.makeInvoker().apply();
      if (seg.equals(MemorySegment.NULL)) {
        return "no last error text";
      }
      return seg.reinterpret(MAX_ERROR_TEXT).getString(0);
    } catch (Throwable t) {
      throw new RuntimeException(t);
    }
  }

  /**
   * Get the list of compatible GPUs based on compute capability >= 7.0 and total
   * memory >= 8GB
   *
   * @return a list of compatible GPUs. See {@link GPUInfo}
   */
  public static List<GPUInfo> compatibleGPUs() throws Throwable {
    return compatibleGPUs(7.0, 8192);
  }

  /**
   * Get the list of compatible GPUs based on given compute capability and total
   * memory
   *
   * @param minComputeCapability the minimum compute capability
   * @param minDeviceMemoryMB    the minimum total available memory in MB
   * @return a list of compatible GPUs. See {@link GPUInfo}
   */
  public static List<GPUInfo> compatibleGPUs(double minComputeCapability, int minDeviceMemoryMB)
      throws Throwable {
    List<GPUInfo> compatibleGPUs = new ArrayList<GPUInfo>();
    double minDeviceMemoryB = Math.pow(2, 20) * minDeviceMemoryMB;
    for (GPUInfo gpuInfo : availableGPUs()) {
      if (gpuInfo.computeCapability() >= minComputeCapability
          && gpuInfo.totalMemory() >= minDeviceMemoryB) {
        compatibleGPUs.add(gpuInfo);
      }
    }
    return compatibleGPUs;
  }

  /**
   * Gets all the available GPUs
   *
   * @return a list of {@link GPUInfo} objects with GPU details
   */
  public static List<GPUInfo> availableGPUs() throws Throwable {
    try (var localArena = Arena.ofConfined()) {

      MemorySegment numGpus = localArena.allocate(C_INT);
      int returnValue = cudaGetDeviceCount(numGpus);
      checkCudaError(returnValue, "cudaGetDeviceCount");

      int numGpuCount = numGpus.get(C_INT, 0);
      List<GPUInfo> gpuInfoArr = new ArrayList<GPUInfo>();

      MemorySegment free = localArena.allocate(size_t);
      MemorySegment total = localArena.allocate(size_t);
      MemorySegment deviceProp = cudaDeviceProp.allocate(localArena);

      for (int i = 0; i < numGpuCount; i++) {

        returnValue = cudaSetDevice(i);
        checkCudaError(returnValue, "cudaSetDevice");

        returnValue = cudaGetDeviceProperties_v2(deviceProp, i);
        checkCudaError(returnValue, "cudaGetDeviceProperties_v2");

        returnValue = cudaMemGetInfo(free, total);
        checkCudaError(returnValue, "cudaMemGetInfo");

        float computeCapability =
            Float.parseFloat(
                cudaDeviceProp.major(deviceProp) + "." + cudaDeviceProp.minor(deviceProp));

        GPUInfo gpuInfo =
            new GPUInfo(
                i,
                cudaDeviceProp.name(deviceProp).getString(0),
                free.get(C_LONG, 0),
                total.get(C_LONG, 0),
                computeCapability);

        gpuInfoArr.add(gpuInfo);
      }
      return gpuInfoArr;
    }
  }

  /**
   * A utility method for getting an instance of {@link MemorySegment} for a
   * {@link String}.
   *
   * @param str the string for the expected {@link MemorySegment}
   * @return an instance of {@link MemorySegment}
   */
  public static MemorySegment buildMemorySegment(Arena arena, String str) {
    StringBuilder sb = new StringBuilder(str).append('\0');
    MemoryLayout stringMemoryLayout = MemoryLayout.sequenceLayout(sb.length(), C_CHAR);
    MemorySegment stringMemorySegment = arena.allocate(stringMemoryLayout);

    for (int i = 0; i < sb.length(); i++) {
      VarHandle varHandle = stringMemoryLayout.varHandle(PathElement.sequenceElement(i));
      varHandle.set(stringMemorySegment, 0L, (byte) sb.charAt(i));
    }
    return stringMemorySegment;
  }

  /**
   * A utility method for building a {@link MemorySegment} for a 1D long array.
   *
   * @param data The 1D long array for which the {@link MemorySegment} is needed
   * @return an instance of {@link MemorySegment}
   */
  public static MemorySegment buildMemorySegment(Arena arena, long[] data) {
    int cells = data.length;
    MemoryLayout dataMemoryLayout = MemoryLayout.sequenceLayout(cells, C_LONG);
    MemorySegment dataMemorySegment = arena.allocate(dataMemoryLayout);
    MemorySegment.copy(data, 0, dataMemorySegment, C_LONG, 0, cells);
    return dataMemorySegment;
  }

  public static MemorySegment buildMemorySegment(Arena arena, byte[] data) {
    int cells = data.length;
    MemoryLayout dataMemoryLayout = MemoryLayout.sequenceLayout(cells, C_CHAR);
    MemorySegment dataMemorySegment = arena.allocate(dataMemoryLayout);
    MemorySegment.copy(data, 0, dataMemorySegment, C_CHAR, 0, cells);
    return dataMemorySegment;
  }

  /**
   * A utility method for building a {@link MemorySegment} for a 2D float array.
   *
   * @param data The 2D float array for which the {@link MemorySegment} is needed
   * @return an instance of {@link MemorySegment}
   */
  public static MemorySegment buildMemorySegment(Arena arena, float[][] data) {
    long rows = data.length;
    long cols = rows > 0 ? data[0].length : 0;
    MemoryLayout dataMemoryLayout = MemoryLayout.sequenceLayout(rows * cols, C_FLOAT);
    MemorySegment dataMemorySegment = arena.allocate(dataMemoryLayout);
    copy(dataMemorySegment, data);
    return dataMemorySegment;
  }

  public static void copy(MemorySegment memorySegment, float[][] data) {
    int rows = data.length;
    int cols = rows > 0 ? data[0].length : 0;
    for (int r = 0; r < rows; r++) {
      MemorySegment.copy(data[r], 0, memorySegment, C_FLOAT, (r * cols * C_FLOAT.byteSize()), cols);
    }
  }

  public static void copy(MemorySegment memorySegment, int[][] data) {
    int rows = data.length;
    int cols = rows > 0 ? data[0].length : 0;
    for (int r = 0; r < rows; r++) {
      MemorySegment.copy(data[r], 0, memorySegment, C_INT, (r * cols * C_INT.byteSize()), cols);
    }
  }

  public static void copy(MemorySegment memorySegment, byte[][] data) {
    int rows = data.length;
    int cols = rows > 0 ? data[0].length : 0;
    for (int r = 0; r < rows; r++) {
      MemorySegment.copy(data[r], 0, memorySegment, C_CHAR, (r * cols * C_CHAR.byteSize()), cols);
    }
  }

  public static BitSet concatenate(BitSet[] arr, int maxSizeOfEachBitSet) {
    BitSet ret = new BitSet(maxSizeOfEachBitSet * arr.length);
    for (int i = 0; i < arr.length; i++) {
      BitSet b = arr[i];
      if (b == null || b.length() == 0) {
        ret.set(i * maxSizeOfEachBitSet, (i + 1) * maxSizeOfEachBitSet);
      } else {
        for (int j = 0; j < maxSizeOfEachBitSet; j++) {
          ret.set(i * maxSizeOfEachBitSet + j, b.get(j));
        }
      }
    }
    return ret;
  }

  /**
   * Helper function for creating a DLManagedTensor instance
   *
   * @param arena      the Arena to use to allocate native memory for this data structure
   * @param data       the data pointer points to the allocated data
   * @param shape      the shape of the tensor
   * @param code       the type code of the matrix elements (e.g. kDLFloat())
   * @param bits       the size in bits of the matrix elements
   * @param deviceType the device where {@code data} is held (kDLCPU() or kDLCUDA())
   * @return DLManagedTensor
   */
  public static MemorySegment prepareTensor(
      Arena arena, MemorySegment data, long[] shape, int code, int bits, int deviceType) {
    return prepareTensor(arena, data, shape, null, code, bits, deviceType);
  }

  /**
   * Helper function for creating a DLManagedTensor instance
   *
   * @param arena      the Arena to use to allocate native memory for this data structure
   * @param data       the data pointer points to the allocated data
   * @param shape      the shape of the tensor
   * @param strides    the strides for each dimension (can be null)
   * @param code       the type code of the matrix elements (e.g. kDLFloat())
   * @param bits       the size in bits of the matrix elements
   * @param deviceType the device where {@code data} is held (kDLCPU() or kDLCUDA())
   * @return DLManagedTensor
   */
  public static MemorySegment prepareTensor(
      Arena arena,
      MemorySegment data,
      long[] shape,
      long[] strides,
      int code,
      int bits,
      int deviceType) {

    MemorySegment managedTensor = DLManagedTensor.allocate(arena);
    MemorySegment tensor = DLTensor.allocate(arena);

    DLTensor.data(tensor, data);

    MemorySegment dlDevice = DLDevice.allocate(arena);
    DLDevice.device_type(dlDevice, deviceType);
    DLTensor.device(tensor, dlDevice);

    MemorySegment dtype = DLDataType.allocate(arena);
    DLDataType.code(dtype, (byte) code);
    DLDataType.bits(dtype, (byte) bits);
    // Number of lanes for vectorized types (1 for scalar types)
    DLDataType.lanes(dtype, (short) 1);
    DLTensor.dtype(tensor, dtype);

    DLTensor.ndim(tensor, shape.length);
    DLTensor.shape(tensor, Util.buildMemorySegment(arena, shape));
    if (strides != null) {
      assert shape.length == strides.length;
      DLTensor.strides(tensor, Util.buildMemorySegment(arena, strides));
    } else {
      DLTensor.strides(tensor, MemorySegment.NULL);
    }

    // Copy tensor information into the DLManagedTensor struct
    DLManagedTensor.dl_tensor(managedTensor, tensor);

    assert bits == DLDataType.bits(DLTensor.dtype(DLManagedTensor.dl_tensor(managedTensor)));
    return managedTensor;
  }
}
