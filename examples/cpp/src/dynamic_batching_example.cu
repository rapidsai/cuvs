/*
 * Copyright (c) 2024, NVIDIA CORPORATION.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include "common.cuh"

#include <cuvs/neighbors/cagra.hpp>
#include <cuvs/neighbors/dynamic_batching.hpp>

#include <raft/core/device_mdarray.hpp>
#include <raft/core/resource/cuda_stream.hpp>
#include <raft/core/resource/cuda_stream_pool.hpp>
#include <raft/core/resources.hpp>
#include <raft/random/make_blobs.cuh>
#include <rmm/cuda_stream_pool.hpp>
#include <rmm/mr/device/device_memory_resource.hpp>
#include <rmm/mr/device/pool_memory_resource.hpp>

#include <array>
#include <chrono>
#include <cstdint>
#include <future>

// A helper to split the dataset into chunks
template <typename DeviceMatrixOrView>
auto slice_matrix(const DeviceMatrixOrView &source,
                  typename DeviceMatrixOrView::index_type offset_rows,
                  typename DeviceMatrixOrView::index_type count_rows) {
  auto n_cols = source.extent(1);
  return raft::make_device_matrix_view<
      typename DeviceMatrixOrView::element_type,
      typename DeviceMatrixOrView::index_type>(
      const_cast<typename DeviceMatrixOrView::element_type *>(
          source.data_handle()) +
          offset_rows * n_cols,
      count_rows, n_cols);
}

// A helper to measure the execution time of a function
template <typename F, typename... Args>
void time_it(std::string label, F f, Args &&...xs) {
  auto start = std::chrono::system_clock::now();
  f(std::forward<Args>(xs)...);
  auto end = std::chrono::system_clock::now();
  auto t = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
  auto t_ms = double(t.count()) / 1000.0;
  std::cout << "[" << label << "] execution time: " << t_ms << " ms"
            << std::endl;
}

/**
 * Wrap waiting on a stream work into an async C++ future object.
 * This is similar to recording and waiting on CUDA events, but in C++11 API.
 */
struct cuda_work_completion_promise {

  cuda_work_completion_promise(const raft::resources &res) {
    auto *promise = new std::promise<void>;
    RAFT_CUDA_TRY(cudaLaunchHostFunc(raft::resource::get_cuda_stream(res),
                                     completion_callback,
                                     reinterpret_cast<void *>(promise)));
    value_ = promise->get_future();
  }

  /**
   * Waiting on the produced `future` object has the same effect as
   * cudaEventSynchronize if an event was recorded at the time of creation of
   * this promise object.
   */
  auto get_future() -> std::future<void> && { return std::move(value_); }

private:
  std::future<void> value_;

  static void completion_callback(void *ptr) {
    auto *promise = reinterpret_cast<std::promise<void> *>(ptr);
    promise->set_value();
    delete promise;
  }
};

void dynamic_batching_example(
    raft::resources const &res,
    raft::device_matrix_view<const float, int64_t> dataset,
    raft::device_matrix_view<const float, int64_t> queries) {
  using namespace cuvs::neighbors;

  // Number of neighbors to search
  int64_t topk = 100;

  // Streaming scenario: maximum number of requests in-flight
  constexpr int64_t kMaxJobs = 1000;
  // Streaming scenario: number of concurrent CUDA streams
  constexpr int64_t kNumWorkerStreams = 5;

  // Split the queries into two subsets to run every experiment twice and thus
  // surface any initialization overheads.
  int64_t n_queries_a = queries.extent(0) / 2;
  int64_t n_queries_b = queries.extent(0) - n_queries_a;

  auto queries_a = slice_matrix(queries, 0, n_queries_a);
  auto queries_b = slice_matrix(queries, n_queries_a, n_queries_b);

  // create output arrays
  auto neighbors =
      raft::make_device_matrix<uint32_t>(res, queries.extent(0), topk);
  auto distances =
      raft::make_device_matrix<float>(res, queries.extent(0), topk);
  // slice them same as queries
  auto neighbors_a = slice_matrix(neighbors, 0, n_queries_a);
  auto distances_a = slice_matrix(distances, 0, n_queries_a);
  auto neighbors_b = slice_matrix(neighbors, n_queries_a, n_queries_b);
  auto distances_b = slice_matrix(distances, n_queries_a, n_queries_b);

  // use default index parameters
  cagra::index_params orig_index_params;

  std::cout << "Building CAGRA index (search graph)" << std::endl;
  auto orig_index = cagra::build(res, orig_index_params, dataset);

  std::cout << "CAGRA index has " << orig_index.size() << " vectors"
            << std::endl;
  std::cout << "CAGRA graph has degree " << orig_index.graph_degree()
            << ", graph size [" << orig_index.graph().extent(0) << ", "
            << orig_index.graph().extent(1) << "]" << std::endl;

  // use default search parameters
  cagra::search_params orig_search_params;
  // get a decent recall by increasing the internal topk list
  orig_search_params.itopk_size = 512;
  orig_search_params.algo = cagra::search_algo::SINGLE_CTA;

  // Set up dynamic batching parameters
  dynamic_batching::index_params dynb_index_params{
      /* default-initializing the parent `neighbors::index_params`
         (not used anyway) */
      {},
      /* Set the K in advance (the batcher needs to allocate buffers) */
      topk,
      /* Configure the number and the size of IO buffers */
      64,
      kNumWorkerStreams};

  // "build" the index (it's a low-cost index wrapping),
  //  that is we need to pass the original index and its search params here
  dynamic_batching::index<float, uint32_t> dynb_index(
      res, dynb_index_params, orig_index, orig_search_params);

  // You can implement job priorities by varying the deadlines of individual
  // requests
  dynamic_batching::search_params dynb_search_params;
  dynb_search_params.dispatch_timeout_ms = 0.1;

  // Define the big-batch setting as a baseline for measuring the throughput.
  auto search_batch_orig =
      [&res, &orig_index, &orig_search_params](
          raft::device_matrix_view<const float, int64_t> queries,
          raft::device_matrix_view<uint32_t, int64_t> neighbors,
          raft::device_matrix_view<float, int64_t> distances) {
        cagra::search(res, orig_search_params, orig_index, queries, neighbors,
                      distances);
        raft::resource::sync_stream(res);
      };

  // Launch the baseline search: check the big-batch performance
  time_it("standard/batch A", search_batch_orig, queries_a, neighbors_a,
          distances_a);
  time_it("standard/batch B", search_batch_orig, queries_b, neighbors_b,
          distances_b);

  // Streaming scenario: prepare concurrent resources
  rmm::cuda_stream_pool worker_streams{kNumWorkerStreams};
  std::vector<raft::resources> resource_pool(0);
  for (int64_t i = 0; i < kNumWorkerStreams; i++) {
    resource_pool.push_back(res);
    raft::resource::set_cuda_stream(resource_pool[i],
                                    worker_streams.get_stream(i));
  }

  // Streaming scenario:
  // send queries one-by-one, with a maximum kMaxJobs in-flight
  auto search_async_orig =
      [&resource_pool, &orig_index, &orig_search_params](
          raft::device_matrix_view<const float, int64_t> queries,
          raft::device_matrix_view<uint32_t, int64_t> neighbors,
          raft::device_matrix_view<float, int64_t> distances) {
        auto work_size = queries.extent(0);
        std::array<std::future<void>, kMaxJobs> futures;
        for (int64_t i = 0; i < work_size + kMaxJobs; i++) {
          // wait for previous job in the same slot to finish
          if (i >= kMaxJobs) {
            futures[i % kMaxJobs].wait();
          }
          // submit a new job
          if (i < work_size) {
            auto &res = resource_pool[i % kNumWorkerStreams];
            cagra::search(res, orig_search_params, orig_index,
                          slice_matrix(queries, i, 1),
                          slice_matrix(neighbors, i, 1),
                          slice_matrix(distances, i, 1));
            futures[i % kMaxJobs] =
                cuda_work_completion_promise(res).get_future();
          }
        }
      };

  // Streaming scenario with dynamic batching:
  // send queries one-by-one, with a maximum kMaxJobs in-flight,
  // yet allow grouping the sequential requests (subject to deadlines)
  auto search_async_dynb =
      [&resource_pool, &dynb_index, &dynb_search_params](
          raft::device_matrix_view<const float, int64_t> queries,
          raft::device_matrix_view<uint32_t, int64_t> neighbors,
          raft::device_matrix_view<float, int64_t> distances) {
        auto work_size = queries.extent(0);
        std::array<std::future<void>, kMaxJobs> futures;
        for (int64_t i = 0; i < work_size + kMaxJobs; i++) {
          // wait for previous job in the same slot to finish
          if (i >= kMaxJobs) {
            futures[i % kMaxJobs].wait();
          }
          // submit a new job
          if (i < work_size) {
            auto &res = resource_pool[i % kNumWorkerStreams];
            dynamic_batching::search(res, dynb_search_params, dynb_index,
                                     slice_matrix(queries, i, 1),
                                     slice_matrix(neighbors, i, 1),
                                     slice_matrix(distances, i, 1));
            futures[i % kMaxJobs] =
                cuda_work_completion_promise(res).get_future();
          }
        }
      };

  // Try to handle the same amount of work in the async setting using the
  // standard implementation.
  time_it("standard/async A", search_async_orig, queries_a, neighbors_a,
          distances_a);
  time_it("standard/async B", search_async_orig, queries_b, neighbors_b,
          distances_b);

  // Do the same using dynamic batching
  time_it("dynamic_batching/async A", search_async_dynb, queries_a, neighbors_a,
          distances_a);
  time_it("dynamic_batching/async B", search_async_dynb, queries_b, neighbors_b,
          distances_b);
}

int main() {
  raft::device_resources res;

  // Set the raft resource to use a pool for internal memory allocations
  // (workspace) and limit the available workspace size.
  raft::resource::set_workspace_to_pool_resource(res,
                                                 12ull * 1024 * 1024 * 1024ull);

  // Create input arrays.
  int64_t n_samples = 1000000;
  int64_t n_dim = 128;
  int64_t n_queries = 10000;
  auto dataset =
      raft::make_device_matrix<float, int64_t>(res, n_samples, n_dim);
  auto queries =
      raft::make_device_matrix<float, int64_t>(res, n_queries, n_dim);
  generate_dataset(res, dataset.view(), queries.view());

  // run the interesting part of the program
  dynamic_batching_example(res, raft::make_const_mdspan(dataset.view()),
                           raft::make_const_mdspan(queries.view()));
}
